{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      obs_mth  bad_ind        uid  td_score  jxl_score  mj_score  rh_score  \\\n0  2018-10-31      0.0  A10000005  0.675349   0.144072  0.186899  0.483640   \n1  2018-07-31      0.0   A1000002  0.825269   0.398688  0.139396  0.843725   \n2  2018-09-30      0.0   A1000011  0.315406   0.629745  0.535854  0.197392   \n3  2018-07-31      0.0  A10000481  0.002386   0.609360  0.366081  0.342243   \n4  2018-07-31      0.0   A1000069  0.406310   0.405352  0.783015  0.563953   \n\n   zzc_score  zcx_score  person_info  finance_info  credit_info  act_info  \n0   0.928328   0.369644    -0.322581      0.023810         0.00  0.217949  \n1   0.605194   0.406122    -0.128677      0.023810         0.00  0.423077  \n2   0.614416   0.320731     0.062660      0.023810         0.10  0.448718  \n3   0.870006   0.288692     0.078853      0.071429         0.05  0.179487  \n4   0.715454   0.512554    -0.261014      0.023810         0.00  0.423077  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>obs_mth</th>\n      <th>bad_ind</th>\n      <th>uid</th>\n      <th>td_score</th>\n      <th>jxl_score</th>\n      <th>mj_score</th>\n      <th>rh_score</th>\n      <th>zzc_score</th>\n      <th>zcx_score</th>\n      <th>person_info</th>\n      <th>finance_info</th>\n      <th>credit_info</th>\n      <th>act_info</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A10000005</td>\n      <td>0.675349</td>\n      <td>0.144072</td>\n      <td>0.186899</td>\n      <td>0.483640</td>\n      <td>0.928328</td>\n      <td>0.369644</td>\n      <td>-0.322581</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.217949</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-07-31</td>\n      <td>0.0</td>\n      <td>A1000002</td>\n      <td>0.825269</td>\n      <td>0.398688</td>\n      <td>0.139396</td>\n      <td>0.843725</td>\n      <td>0.605194</td>\n      <td>0.406122</td>\n      <td>-0.128677</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.423077</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-09-30</td>\n      <td>0.0</td>\n      <td>A1000011</td>\n      <td>0.315406</td>\n      <td>0.629745</td>\n      <td>0.535854</td>\n      <td>0.197392</td>\n      <td>0.614416</td>\n      <td>0.320731</td>\n      <td>0.062660</td>\n      <td>0.023810</td>\n      <td>0.10</td>\n      <td>0.448718</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-07-31</td>\n      <td>0.0</td>\n      <td>A10000481</td>\n      <td>0.002386</td>\n      <td>0.609360</td>\n      <td>0.366081</td>\n      <td>0.342243</td>\n      <td>0.870006</td>\n      <td>0.288692</td>\n      <td>0.078853</td>\n      <td>0.071429</td>\n      <td>0.05</td>\n      <td>0.179487</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-07-31</td>\n      <td>0.0</td>\n      <td>A1000069</td>\n      <td>0.406310</td>\n      <td>0.405352</td>\n      <td>0.783015</td>\n      <td>0.563953</td>\n      <td>0.715454</td>\n      <td>0.512554</td>\n      <td>-0.261014</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.423077</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "data = pd.read_csv('../data/Bcard.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "2018-07-31    34030\n2018-06-30    20565\n2018-11-30    15975\n2018-10-31    14527\n2018-09-30    10709\nName: obs_mth, dtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['obs_mth'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df_train = data[data['obs_mth'] != '2018-11-30']\n",
    "val = data[data['obs_mth'] == '2018-11-30']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "          obs_mth  bad_ind                     uid  td_score  jxl_score  \\\n0      2018-10-31      0.0               A10000005  0.675349   0.144072   \n1      2018-07-31      0.0                A1000002  0.825269   0.398688   \n2      2018-09-30      0.0                A1000011  0.315406   0.629745   \n3      2018-07-31      0.0               A10000481  0.002386   0.609360   \n4      2018-07-31      0.0                A1000069  0.406310   0.405352   \n...           ...      ...                     ...       ...        ...   \n79826  2018-09-30      0.0  Ab99_96436392005147255  0.905578   0.927706   \n79827  2018-10-31      0.0  Ab99_96436392005205003  0.639265   0.219267   \n79828  2018-10-31      0.0  Ab99_96436392005412387  0.355061   0.837747   \n79829  2018-10-31      0.0  Ab99_96436392006241624  0.797237   0.501238   \n79830  2018-10-31      0.0  Ab99_96436392007495598  0.990920   0.833572   \n\n       mj_score  rh_score  zzc_score  zcx_score  person_info  finance_info  \\\n0      0.186899  0.483640   0.928328   0.369644    -0.322581      0.023810   \n1      0.139396  0.843725   0.605194   0.406122    -0.128677      0.023810   \n2      0.535854  0.197392   0.614416   0.320731     0.062660      0.023810   \n3      0.366081  0.342243   0.870006   0.288692     0.078853      0.071429   \n4      0.783015  0.563953   0.715454   0.512554    -0.261014      0.023810   \n...         ...       ...        ...        ...          ...           ...   \n79826  0.994447  0.315842   0.959443   0.042640     0.078853      0.071429   \n79827  0.845014  0.751332   0.275557   0.902642     0.078853      0.023810   \n79828  0.931882  0.442463   0.579277   0.740754     0.078853      0.023810   \n79829  0.490850  0.592068   0.479618   0.859870     0.078853      0.023810   \n79830  0.993425  0.732783   0.925975   0.395655     0.078853      0.023810   \n\n       credit_info  act_info  \n0             0.00  0.217949  \n1             0.00  0.423077  \n2             0.10  0.448718  \n3             0.05  0.179487  \n4             0.00  0.423077  \n...            ...       ...  \n79826         0.13  0.076923  \n79827         0.00  0.076923  \n79828         0.02  0.076923  \n79829         0.00  0.076923  \n79830         0.00  0.076923  \n\n[79831 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>obs_mth</th>\n      <th>bad_ind</th>\n      <th>uid</th>\n      <th>td_score</th>\n      <th>jxl_score</th>\n      <th>mj_score</th>\n      <th>rh_score</th>\n      <th>zzc_score</th>\n      <th>zcx_score</th>\n      <th>person_info</th>\n      <th>finance_info</th>\n      <th>credit_info</th>\n      <th>act_info</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A10000005</td>\n      <td>0.675349</td>\n      <td>0.144072</td>\n      <td>0.186899</td>\n      <td>0.483640</td>\n      <td>0.928328</td>\n      <td>0.369644</td>\n      <td>-0.322581</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.217949</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2018-07-31</td>\n      <td>0.0</td>\n      <td>A1000002</td>\n      <td>0.825269</td>\n      <td>0.398688</td>\n      <td>0.139396</td>\n      <td>0.843725</td>\n      <td>0.605194</td>\n      <td>0.406122</td>\n      <td>-0.128677</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.423077</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2018-09-30</td>\n      <td>0.0</td>\n      <td>A1000011</td>\n      <td>0.315406</td>\n      <td>0.629745</td>\n      <td>0.535854</td>\n      <td>0.197392</td>\n      <td>0.614416</td>\n      <td>0.320731</td>\n      <td>0.062660</td>\n      <td>0.023810</td>\n      <td>0.10</td>\n      <td>0.448718</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2018-07-31</td>\n      <td>0.0</td>\n      <td>A10000481</td>\n      <td>0.002386</td>\n      <td>0.609360</td>\n      <td>0.366081</td>\n      <td>0.342243</td>\n      <td>0.870006</td>\n      <td>0.288692</td>\n      <td>0.078853</td>\n      <td>0.071429</td>\n      <td>0.05</td>\n      <td>0.179487</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2018-07-31</td>\n      <td>0.0</td>\n      <td>A1000069</td>\n      <td>0.406310</td>\n      <td>0.405352</td>\n      <td>0.783015</td>\n      <td>0.563953</td>\n      <td>0.715454</td>\n      <td>0.512554</td>\n      <td>-0.261014</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.423077</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>79826</th>\n      <td>2018-09-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436392005147255</td>\n      <td>0.905578</td>\n      <td>0.927706</td>\n      <td>0.994447</td>\n      <td>0.315842</td>\n      <td>0.959443</td>\n      <td>0.042640</td>\n      <td>0.078853</td>\n      <td>0.071429</td>\n      <td>0.13</td>\n      <td>0.076923</td>\n    </tr>\n    <tr>\n      <th>79827</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>Ab99_96436392005205003</td>\n      <td>0.639265</td>\n      <td>0.219267</td>\n      <td>0.845014</td>\n      <td>0.751332</td>\n      <td>0.275557</td>\n      <td>0.902642</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.076923</td>\n    </tr>\n    <tr>\n      <th>79828</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>Ab99_96436392005412387</td>\n      <td>0.355061</td>\n      <td>0.837747</td>\n      <td>0.931882</td>\n      <td>0.442463</td>\n      <td>0.579277</td>\n      <td>0.740754</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.02</td>\n      <td>0.076923</td>\n    </tr>\n    <tr>\n      <th>79829</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>Ab99_96436392006241624</td>\n      <td>0.797237</td>\n      <td>0.501238</td>\n      <td>0.490850</td>\n      <td>0.592068</td>\n      <td>0.479618</td>\n      <td>0.859870</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.076923</td>\n    </tr>\n    <tr>\n      <th>79830</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>Ab99_96436392007495598</td>\n      <td>0.990920</td>\n      <td>0.833572</td>\n      <td>0.993425</td>\n      <td>0.732783</td>\n      <td>0.925975</td>\n      <td>0.395655</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.076923</td>\n    </tr>\n  </tbody>\n</table>\n<p>79831 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "          obs_mth  bad_ind                     uid  td_score  jxl_score  \\\n79831  2018-11-30      0.0               A10002345  0.123276   0.872117   \n79832  2018-11-30      0.0               A10003755  0.462460   0.157643   \n79833  2018-11-30      0.0                A1000756  0.812642   0.400040   \n79834  2018-11-30      0.0                 A100085  0.007039   0.396036   \n79835  2018-11-30      0.0               A10008856  0.078063   0.291289   \n...           ...      ...                     ...       ...        ...   \n95801  2018-11-30      0.0  Ab99_96436391998107976  0.890233   0.442687   \n95802  2018-11-30      0.0  Ab99_96436391998176292  0.161840   0.495766   \n95803  2018-11-30      0.0  Ab99_96436391998322771  0.746522   0.732739   \n95804  2018-11-30      0.0  Ab99_96436391998973383  0.176846   0.749610   \n95805  2018-11-30      0.0  Ab99_96436392001380983  0.417920   0.650343   \n\n       mj_score  rh_score  zzc_score  zcx_score  person_info  finance_info  \\\n79831  0.723560  0.759074   0.184735   0.080376    -0.053718      0.047619   \n79832  0.762271  0.481466   0.967006   0.780087     0.013863      0.023810   \n79833  0.280942  0.099454   0.942880   0.588936     0.078853      0.023810   \n79834  0.857868  0.882255   0.345511   0.419969    -0.053718      0.047619   \n79835  0.654864  0.528708   0.754482   0.732534     0.013863      0.023810   \n...         ...       ...        ...        ...          ...           ...   \n95801  0.802687  0.776982   0.638971   0.605522     0.078853      0.142857   \n95802  0.085750  0.536738   0.596144   0.132972     0.078853      0.023810   \n95803  0.025475  0.831805   0.642904   0.029297     0.078853      0.023810   \n95804  0.933879  0.506921   0.867099   0.751643     0.078853      0.023810   \n95805  0.985863  0.374100   0.330634   0.596833     0.078853      0.071429   \n\n       credit_info  act_info  \n79831         1.00  0.230769  \n79832         0.00  0.230769  \n79833         0.02  0.474359  \n79834         0.02  0.666667  \n79835         0.00  0.230769  \n...            ...       ...  \n95801         0.25  0.076923  \n95802         0.00  0.076923  \n95803         0.00  0.076923  \n95804         0.02  0.076923  \n95805         0.62  0.076923  \n\n[15975 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>obs_mth</th>\n      <th>bad_ind</th>\n      <th>uid</th>\n      <th>td_score</th>\n      <th>jxl_score</th>\n      <th>mj_score</th>\n      <th>rh_score</th>\n      <th>zzc_score</th>\n      <th>zcx_score</th>\n      <th>person_info</th>\n      <th>finance_info</th>\n      <th>credit_info</th>\n      <th>act_info</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>79831</th>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A10002345</td>\n      <td>0.123276</td>\n      <td>0.872117</td>\n      <td>0.723560</td>\n      <td>0.759074</td>\n      <td>0.184735</td>\n      <td>0.080376</td>\n      <td>-0.053718</td>\n      <td>0.047619</td>\n      <td>1.00</td>\n      <td>0.230769</td>\n    </tr>\n    <tr>\n      <th>79832</th>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A10003755</td>\n      <td>0.462460</td>\n      <td>0.157643</td>\n      <td>0.762271</td>\n      <td>0.481466</td>\n      <td>0.967006</td>\n      <td>0.780087</td>\n      <td>0.013863</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.230769</td>\n    </tr>\n    <tr>\n      <th>79833</th>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A1000756</td>\n      <td>0.812642</td>\n      <td>0.400040</td>\n      <td>0.280942</td>\n      <td>0.099454</td>\n      <td>0.942880</td>\n      <td>0.588936</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.02</td>\n      <td>0.474359</td>\n    </tr>\n    <tr>\n      <th>79834</th>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A100085</td>\n      <td>0.007039</td>\n      <td>0.396036</td>\n      <td>0.857868</td>\n      <td>0.882255</td>\n      <td>0.345511</td>\n      <td>0.419969</td>\n      <td>-0.053718</td>\n      <td>0.047619</td>\n      <td>0.02</td>\n      <td>0.666667</td>\n    </tr>\n    <tr>\n      <th>79835</th>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A10008856</td>\n      <td>0.078063</td>\n      <td>0.291289</td>\n      <td>0.654864</td>\n      <td>0.528708</td>\n      <td>0.754482</td>\n      <td>0.732534</td>\n      <td>0.013863</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.230769</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95801</th>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436391998107976</td>\n      <td>0.890233</td>\n      <td>0.442687</td>\n      <td>0.802687</td>\n      <td>0.776982</td>\n      <td>0.638971</td>\n      <td>0.605522</td>\n      <td>0.078853</td>\n      <td>0.142857</td>\n      <td>0.25</td>\n      <td>0.076923</td>\n    </tr>\n    <tr>\n      <th>95802</th>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436391998176292</td>\n      <td>0.161840</td>\n      <td>0.495766</td>\n      <td>0.085750</td>\n      <td>0.536738</td>\n      <td>0.596144</td>\n      <td>0.132972</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.076923</td>\n    </tr>\n    <tr>\n      <th>95803</th>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436391998322771</td>\n      <td>0.746522</td>\n      <td>0.732739</td>\n      <td>0.025475</td>\n      <td>0.831805</td>\n      <td>0.642904</td>\n      <td>0.029297</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.076923</td>\n    </tr>\n    <tr>\n      <th>95804</th>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436391998973383</td>\n      <td>0.176846</td>\n      <td>0.749610</td>\n      <td>0.933879</td>\n      <td>0.506921</td>\n      <td>0.867099</td>\n      <td>0.751643</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.02</td>\n      <td>0.076923</td>\n    </tr>\n    <tr>\n      <th>95805</th>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436392001380983</td>\n      <td>0.417920</td>\n      <td>0.650343</td>\n      <td>0.985863</td>\n      <td>0.374100</td>\n      <td>0.330634</td>\n      <td>0.596833</td>\n      <td>0.078853</td>\n      <td>0.071429</td>\n      <td>0.62</td>\n      <td>0.076923</td>\n    </tr>\n  </tbody>\n</table>\n<p>15975 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "          obs_mth  bad_ind        uid  td_score  jxl_score  mj_score  \\\n0      2018-10-31      0.0  A10000005  0.675349   0.144072  0.186899   \n33407  2018-10-31      0.0   A2810176  0.146055   0.079922  0.250568   \n33383  2018-10-31      0.0   A2807687  0.551366   0.300781  0.225007   \n33379  2018-10-31      0.0   A2807232  0.708547   0.769513  0.928457   \n33376  2018-10-31      0.0   A2806932  0.482248   0.116658  0.286273   \n...           ...      ...        ...       ...        ...       ...   \n47112  2018-06-30      0.0   A4233575  0.723610   0.857182  0.117430   \n47109  2018-06-30      0.0   A4233356  0.636472   0.432827  0.423410   \n47106  2018-06-30      0.0   A4232739  0.911809   0.871835  0.073466   \n47104  2018-06-30      0.0   A4232571  0.333640   0.771295  0.678957   \n39915  2018-06-30      0.0    A345019  0.510283   0.545837  0.066670   \n\n       rh_score  zzc_score  zcx_score  person_info  finance_info  credit_info  \\\n0      0.483640   0.928328   0.369644    -0.322581      0.023810         0.00   \n33407  0.045240   0.766906   0.413713     0.013863      0.023810         0.00   \n33383  0.045447   0.735733   0.684182    -0.261014      0.071429         0.03   \n33379  0.739716   0.947453   0.361551    -0.128677      0.047619         0.00   \n33376  0.056618   0.047024   0.890433     0.078853      0.047619         0.00   \n...         ...        ...        ...          ...           ...          ...   \n47112  0.949779   0.280677   0.431667     0.078853      0.023810         0.00   \n47109  0.516783   0.678267   0.788935    -0.322581      0.023810         0.00   \n47106  0.433998   0.747908   0.962329     0.078853      0.023810         0.00   \n47104  0.305642   0.231494   0.020833     0.062660      0.047619         0.00   \n39915  0.257507   0.288563   0.628067    -0.322581      0.023810         0.00   \n\n       act_info  \n0      0.217949  \n33407  0.269231  \n33383  0.269231  \n33379  0.269231  \n33376  0.269231  \n...         ...  \n47112  0.166667  \n47109  0.166667  \n47106  0.166667  \n47104  0.166667  \n39915  0.538462  \n\n[79831 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>obs_mth</th>\n      <th>bad_ind</th>\n      <th>uid</th>\n      <th>td_score</th>\n      <th>jxl_score</th>\n      <th>mj_score</th>\n      <th>rh_score</th>\n      <th>zzc_score</th>\n      <th>zcx_score</th>\n      <th>person_info</th>\n      <th>finance_info</th>\n      <th>credit_info</th>\n      <th>act_info</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A10000005</td>\n      <td>0.675349</td>\n      <td>0.144072</td>\n      <td>0.186899</td>\n      <td>0.483640</td>\n      <td>0.928328</td>\n      <td>0.369644</td>\n      <td>-0.322581</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.217949</td>\n    </tr>\n    <tr>\n      <th>33407</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A2810176</td>\n      <td>0.146055</td>\n      <td>0.079922</td>\n      <td>0.250568</td>\n      <td>0.045240</td>\n      <td>0.766906</td>\n      <td>0.413713</td>\n      <td>0.013863</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.269231</td>\n    </tr>\n    <tr>\n      <th>33383</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A2807687</td>\n      <td>0.551366</td>\n      <td>0.300781</td>\n      <td>0.225007</td>\n      <td>0.045447</td>\n      <td>0.735733</td>\n      <td>0.684182</td>\n      <td>-0.261014</td>\n      <td>0.071429</td>\n      <td>0.03</td>\n      <td>0.269231</td>\n    </tr>\n    <tr>\n      <th>33379</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A2807232</td>\n      <td>0.708547</td>\n      <td>0.769513</td>\n      <td>0.928457</td>\n      <td>0.739716</td>\n      <td>0.947453</td>\n      <td>0.361551</td>\n      <td>-0.128677</td>\n      <td>0.047619</td>\n      <td>0.00</td>\n      <td>0.269231</td>\n    </tr>\n    <tr>\n      <th>33376</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A2806932</td>\n      <td>0.482248</td>\n      <td>0.116658</td>\n      <td>0.286273</td>\n      <td>0.056618</td>\n      <td>0.047024</td>\n      <td>0.890433</td>\n      <td>0.078853</td>\n      <td>0.047619</td>\n      <td>0.00</td>\n      <td>0.269231</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>47112</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A4233575</td>\n      <td>0.723610</td>\n      <td>0.857182</td>\n      <td>0.117430</td>\n      <td>0.949779</td>\n      <td>0.280677</td>\n      <td>0.431667</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <th>47109</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A4233356</td>\n      <td>0.636472</td>\n      <td>0.432827</td>\n      <td>0.423410</td>\n      <td>0.516783</td>\n      <td>0.678267</td>\n      <td>0.788935</td>\n      <td>-0.322581</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <th>47106</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A4232739</td>\n      <td>0.911809</td>\n      <td>0.871835</td>\n      <td>0.073466</td>\n      <td>0.433998</td>\n      <td>0.747908</td>\n      <td>0.962329</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <th>47104</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A4232571</td>\n      <td>0.333640</td>\n      <td>0.771295</td>\n      <td>0.678957</td>\n      <td>0.305642</td>\n      <td>0.231494</td>\n      <td>0.020833</td>\n      <td>0.062660</td>\n      <td>0.047619</td>\n      <td>0.00</td>\n      <td>0.166667</td>\n    </tr>\n    <tr>\n      <th>39915</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A345019</td>\n      <td>0.510283</td>\n      <td>0.545837</td>\n      <td>0.066670</td>\n      <td>0.257507</td>\n      <td>0.288563</td>\n      <td>0.628067</td>\n      <td>-0.322581</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.538462</td>\n    </tr>\n  </tbody>\n</table>\n<p>79831 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.sort_values(by=['obs_mth'],ascending=False)\n",
    "df_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "          obs_mth  bad_ind        uid  td_score  jxl_score  mj_score  \\\n0      2018-10-31      0.0  A10000005  0.675349   0.144072  0.186899   \n33407  2018-10-31      0.0   A2810176  0.146055   0.079922  0.250568   \n33383  2018-10-31      0.0   A2807687  0.551366   0.300781  0.225007   \n33379  2018-10-31      0.0   A2807232  0.708547   0.769513  0.928457   \n33376  2018-10-31      0.0   A2806932  0.482248   0.116658  0.286273   \n...           ...      ...        ...       ...        ...       ...   \n47112  2018-06-30      0.0   A4233575  0.723610   0.857182  0.117430   \n47109  2018-06-30      0.0   A4233356  0.636472   0.432827  0.423410   \n47106  2018-06-30      0.0   A4232739  0.911809   0.871835  0.073466   \n47104  2018-06-30      0.0   A4232571  0.333640   0.771295  0.678957   \n39915  2018-06-30      0.0    A345019  0.510283   0.545837  0.066670   \n\n       rh_score  zzc_score  zcx_score  person_info  finance_info  credit_info  \\\n0      0.483640   0.928328   0.369644    -0.322581      0.023810         0.00   \n33407  0.045240   0.766906   0.413713     0.013863      0.023810         0.00   \n33383  0.045447   0.735733   0.684182    -0.261014      0.071429         0.03   \n33379  0.739716   0.947453   0.361551    -0.128677      0.047619         0.00   \n33376  0.056618   0.047024   0.890433     0.078853      0.047619         0.00   \n...         ...        ...        ...          ...           ...          ...   \n47112  0.949779   0.280677   0.431667     0.078853      0.023810         0.00   \n47109  0.516783   0.678267   0.788935    -0.322581      0.023810         0.00   \n47106  0.433998   0.747908   0.962329     0.078853      0.023810         0.00   \n47104  0.305642   0.231494   0.020833     0.062660      0.047619         0.00   \n39915  0.257507   0.288563   0.628067    -0.322581      0.023810         0.00   \n\n       act_info   rank  \n0      0.217949      0  \n33407  0.269231      1  \n33383  0.269231      2  \n33379  0.269231      3  \n33376  0.269231      4  \n...         ...    ...  \n47112  0.166667  79826  \n47109  0.166667  79827  \n47106  0.166667  79828  \n47104  0.166667  79829  \n39915  0.538462  79830  \n\n[79831 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>obs_mth</th>\n      <th>bad_ind</th>\n      <th>uid</th>\n      <th>td_score</th>\n      <th>jxl_score</th>\n      <th>mj_score</th>\n      <th>rh_score</th>\n      <th>zzc_score</th>\n      <th>zcx_score</th>\n      <th>person_info</th>\n      <th>finance_info</th>\n      <th>credit_info</th>\n      <th>act_info</th>\n      <th>rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A10000005</td>\n      <td>0.675349</td>\n      <td>0.144072</td>\n      <td>0.186899</td>\n      <td>0.483640</td>\n      <td>0.928328</td>\n      <td>0.369644</td>\n      <td>-0.322581</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.217949</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33407</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A2810176</td>\n      <td>0.146055</td>\n      <td>0.079922</td>\n      <td>0.250568</td>\n      <td>0.045240</td>\n      <td>0.766906</td>\n      <td>0.413713</td>\n      <td>0.013863</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.269231</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33383</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A2807687</td>\n      <td>0.551366</td>\n      <td>0.300781</td>\n      <td>0.225007</td>\n      <td>0.045447</td>\n      <td>0.735733</td>\n      <td>0.684182</td>\n      <td>-0.261014</td>\n      <td>0.071429</td>\n      <td>0.03</td>\n      <td>0.269231</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>33379</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A2807232</td>\n      <td>0.708547</td>\n      <td>0.769513</td>\n      <td>0.928457</td>\n      <td>0.739716</td>\n      <td>0.947453</td>\n      <td>0.361551</td>\n      <td>-0.128677</td>\n      <td>0.047619</td>\n      <td>0.00</td>\n      <td>0.269231</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>33376</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A2806932</td>\n      <td>0.482248</td>\n      <td>0.116658</td>\n      <td>0.286273</td>\n      <td>0.056618</td>\n      <td>0.047024</td>\n      <td>0.890433</td>\n      <td>0.078853</td>\n      <td>0.047619</td>\n      <td>0.00</td>\n      <td>0.269231</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>47112</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A4233575</td>\n      <td>0.723610</td>\n      <td>0.857182</td>\n      <td>0.117430</td>\n      <td>0.949779</td>\n      <td>0.280677</td>\n      <td>0.431667</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.166667</td>\n      <td>79826</td>\n    </tr>\n    <tr>\n      <th>47109</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A4233356</td>\n      <td>0.636472</td>\n      <td>0.432827</td>\n      <td>0.423410</td>\n      <td>0.516783</td>\n      <td>0.678267</td>\n      <td>0.788935</td>\n      <td>-0.322581</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.166667</td>\n      <td>79827</td>\n    </tr>\n    <tr>\n      <th>47106</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A4232739</td>\n      <td>0.911809</td>\n      <td>0.871835</td>\n      <td>0.073466</td>\n      <td>0.433998</td>\n      <td>0.747908</td>\n      <td>0.962329</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.166667</td>\n      <td>79828</td>\n    </tr>\n    <tr>\n      <th>47104</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A4232571</td>\n      <td>0.333640</td>\n      <td>0.771295</td>\n      <td>0.678957</td>\n      <td>0.305642</td>\n      <td>0.231494</td>\n      <td>0.020833</td>\n      <td>0.062660</td>\n      <td>0.047619</td>\n      <td>0.00</td>\n      <td>0.166667</td>\n      <td>79829</td>\n    </tr>\n    <tr>\n      <th>39915</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A345019</td>\n      <td>0.510283</td>\n      <td>0.545837</td>\n      <td>0.066670</td>\n      <td>0.257507</td>\n      <td>0.288563</td>\n      <td>0.628067</td>\n      <td>-0.322581</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.538462</td>\n      <td>79830</td>\n    </tr>\n  </tbody>\n</table>\n<p>79831 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对训练集数据进行切分，为了更好地训练模型\n",
    "df_train['rank'] = [i for i in range(df_train.shape[0])]\n",
    "df_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "14"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "          obs_mth  bad_ind        uid  td_score  jxl_score  mj_score  \\\n0      2018-10-31      0.0  A10000005  0.675349   0.144072  0.186899   \n33407  2018-10-31      0.0   A2810176  0.146055   0.079922  0.250568   \n33383  2018-10-31      0.0   A2807687  0.551366   0.300781  0.225007   \n33379  2018-10-31      0.0   A2807232  0.708547   0.769513  0.928457   \n33376  2018-10-31      0.0   A2806932  0.482248   0.116658  0.286273   \n...           ...      ...        ...       ...        ...       ...   \n47112  2018-06-30      0.0   A4233575  0.723610   0.857182  0.117430   \n47109  2018-06-30      0.0   A4233356  0.636472   0.432827  0.423410   \n47106  2018-06-30      0.0   A4232739  0.911809   0.871835  0.073466   \n47104  2018-06-30      0.0   A4232571  0.333640   0.771295  0.678957   \n39915  2018-06-30      0.0    A345019  0.510283   0.545837  0.066670   \n\n       rh_score  zzc_score  zcx_score  person_info  finance_info  credit_info  \\\n0      0.483640   0.928328   0.369644    -0.322581      0.023810         0.00   \n33407  0.045240   0.766906   0.413713     0.013863      0.023810         0.00   \n33383  0.045447   0.735733   0.684182    -0.261014      0.071429         0.03   \n33379  0.739716   0.947453   0.361551    -0.128677      0.047619         0.00   \n33376  0.056618   0.047024   0.890433     0.078853      0.047619         0.00   \n...         ...        ...        ...          ...           ...          ...   \n47112  0.949779   0.280677   0.431667     0.078853      0.023810         0.00   \n47109  0.516783   0.678267   0.788935    -0.322581      0.023810         0.00   \n47106  0.433998   0.747908   0.962329     0.078853      0.023810         0.00   \n47104  0.305642   0.231494   0.020833     0.062660      0.047619         0.00   \n39915  0.257507   0.288563   0.628067    -0.322581      0.023810         0.00   \n\n       act_info rank  \n0      0.217949    0  \n33407  0.269231    0  \n33383  0.269231    0  \n33379  0.269231    0  \n33376  0.269231    0  \n...         ...  ...  \n47112  0.166667    4  \n47109  0.166667    4  \n47106  0.166667    4  \n47104  0.166667    4  \n39915  0.538462    4  \n\n[79831 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>obs_mth</th>\n      <th>bad_ind</th>\n      <th>uid</th>\n      <th>td_score</th>\n      <th>jxl_score</th>\n      <th>mj_score</th>\n      <th>rh_score</th>\n      <th>zzc_score</th>\n      <th>zcx_score</th>\n      <th>person_info</th>\n      <th>finance_info</th>\n      <th>credit_info</th>\n      <th>act_info</th>\n      <th>rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A10000005</td>\n      <td>0.675349</td>\n      <td>0.144072</td>\n      <td>0.186899</td>\n      <td>0.483640</td>\n      <td>0.928328</td>\n      <td>0.369644</td>\n      <td>-0.322581</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.217949</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33407</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A2810176</td>\n      <td>0.146055</td>\n      <td>0.079922</td>\n      <td>0.250568</td>\n      <td>0.045240</td>\n      <td>0.766906</td>\n      <td>0.413713</td>\n      <td>0.013863</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.269231</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33383</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A2807687</td>\n      <td>0.551366</td>\n      <td>0.300781</td>\n      <td>0.225007</td>\n      <td>0.045447</td>\n      <td>0.735733</td>\n      <td>0.684182</td>\n      <td>-0.261014</td>\n      <td>0.071429</td>\n      <td>0.03</td>\n      <td>0.269231</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33379</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A2807232</td>\n      <td>0.708547</td>\n      <td>0.769513</td>\n      <td>0.928457</td>\n      <td>0.739716</td>\n      <td>0.947453</td>\n      <td>0.361551</td>\n      <td>-0.128677</td>\n      <td>0.047619</td>\n      <td>0.00</td>\n      <td>0.269231</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33376</th>\n      <td>2018-10-31</td>\n      <td>0.0</td>\n      <td>A2806932</td>\n      <td>0.482248</td>\n      <td>0.116658</td>\n      <td>0.286273</td>\n      <td>0.056618</td>\n      <td>0.047024</td>\n      <td>0.890433</td>\n      <td>0.078853</td>\n      <td>0.047619</td>\n      <td>0.00</td>\n      <td>0.269231</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>47112</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A4233575</td>\n      <td>0.723610</td>\n      <td>0.857182</td>\n      <td>0.117430</td>\n      <td>0.949779</td>\n      <td>0.280677</td>\n      <td>0.431667</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.166667</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>47109</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A4233356</td>\n      <td>0.636472</td>\n      <td>0.432827</td>\n      <td>0.423410</td>\n      <td>0.516783</td>\n      <td>0.678267</td>\n      <td>0.788935</td>\n      <td>-0.322581</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.166667</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>47106</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A4232739</td>\n      <td>0.911809</td>\n      <td>0.871835</td>\n      <td>0.073466</td>\n      <td>0.433998</td>\n      <td>0.747908</td>\n      <td>0.962329</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.166667</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>47104</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A4232571</td>\n      <td>0.333640</td>\n      <td>0.771295</td>\n      <td>0.678957</td>\n      <td>0.305642</td>\n      <td>0.231494</td>\n      <td>0.020833</td>\n      <td>0.062660</td>\n      <td>0.047619</td>\n      <td>0.00</td>\n      <td>0.166667</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>39915</th>\n      <td>2018-06-30</td>\n      <td>0.0</td>\n      <td>A345019</td>\n      <td>0.510283</td>\n      <td>0.545837</td>\n      <td>0.066670</td>\n      <td>0.257507</td>\n      <td>0.288563</td>\n      <td>0.628067</td>\n      <td>-0.322581</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.538462</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>79831 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['rank'] = pd.cut(df_train['rank'],bins=5,labels=[i for i in range(5)])\n",
    "df_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "0    15967\n1    15966\n2    15966\n3    15966\n4    15966\nName: rank, dtype: int64"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['rank'].value_counts()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "79831"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* LightGBM算法构建"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def lgb_test(train_x,train_y,test_x,test_y):\n",
    "    # 创建lgb对象\n",
    "\tclf =lgb.LGBMClassifier(boosting_type = 'gbdt',objective = 'binary',metric = 'auc',\n",
    "\t\tlearning_rate = 0.3,n_estimators = 100,max_depth = 3,num_leaves = 20,\n",
    "\t\tmax_bin = 45,min_data_in_leaf = 6,bagging_fraction = 0.6,bagging_freq = 0,\n",
    "\t\tfeature_fraction = 0.8)\n",
    "    # 使用这个对象训练lgb模型\n",
    "\tclf.fit(train_x,train_y,eval_set = [(train_x,train_y),(test_x,test_y)],eval_metric = 'auc')\n",
    "    # 返回训练好的lgb模型, 返回最佳的分数\n",
    "\treturn clf,clf.best_score_['valid_1']['auc']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "feature_list = ['td_score', 'jxl_score', 'mj_score','rh_score', 'zzc_score', 'zcx_score', 'person_info', 'finance_info','credit_info', 'act_info']\n",
    "feature_importance_lst = []\n",
    "ks_train_lst = []\n",
    "ks_test_lst = []\n",
    "auc_list = []\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Number of positive: 1054, number of negative: 62810\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 390\n",
      "[LightGBM] [Info] Number of data points in the train set: 63864, number of used features: 10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.016504 -> initscore=-4.087522\n",
      "[LightGBM] [Info] Start training from score -4.087522\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Number of positive: 1103, number of negative: 62762\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 392\n",
      "[LightGBM] [Info] Number of data points in the train set: 63865, number of used features: 10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017271 -> initscore=-4.041316\n",
      "[LightGBM] [Info] Start training from score -4.041316\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Number of positive: 1279, number of negative: 62586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 395\n",
      "[LightGBM] [Info] Number of data points in the train set: 63865, number of used features: 10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020027 -> initscore=-3.890463\n",
      "[LightGBM] [Info] Start training from score -3.890463\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Number of positive: 1258, number of negative: 62607\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 395\n",
      "[LightGBM] [Info] Number of data points in the train set: 63865, number of used features: 10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019698 -> initscore=-3.907354\n",
      "[LightGBM] [Info] Start training from score -3.907354\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Number of positive: 1186, number of negative: 62679\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 395\n",
      "[LightGBM] [Info] Number of data points in the train set: 63865, number of used features: 10\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.018570 -> initscore=-3.967440\n",
      "[LightGBM] [Info] Start training from score -3.967440\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n"
     ]
    }
   ],
   "source": [
    "for rk in range(5):\n",
    "    #测试数据集\n",
    "    ttest = df_train[df_train['rank'] == rk]\n",
    "    #训练数据集\n",
    "    ttrain = df_train[df_train['rank'] != rk]\n",
    "\n",
    "    #准备训练集的x和y\n",
    "    train_x = ttrain[feature_list]\n",
    "    train_y = ttrain['bad_ind']\n",
    "\n",
    "    #准备测试集的x和y\n",
    "    test_x = ttest[feature_list]\n",
    "    test_y = ttest['bad_ind']\n",
    "\n",
    "    #模型训练\n",
    "    model,auc = lgb_test(train_x,train_y,test_x,test_y)\n",
    "\n",
    "    #保存结果\n",
    "    feature_importance_df = pd.DataFrame({'name':model.feature_name_,'importance':model.feature_importances_})\\\n",
    "        .set_index('name')\n",
    "    #保存刚刚的特征重要性结果\n",
    "    feature_importance_lst.append(feature_importance_df)\n",
    "    #保存auc值\n",
    "    auc_list.append(auc)\n",
    "\n",
    "    #模型预测\n",
    "    y_pred_train = model.predict_proba(train_x)[:,1]\n",
    "    y_pred_test = model.predict_proba(test_x)[:,1]\n",
    "\n",
    "    #获取TPR，FPR\n",
    "    fpr_train,tpr_train,_ = roc_curve(train_y,y_pred_train)\n",
    "    fpr_test,tpr_test,_ = roc_curve(test_y,y_pred_test)\n",
    "\n",
    "    #计算KS\n",
    "    train_ks = abs(fpr_train - tpr_train).max()\n",
    "    test_ks = abs(fpr_test - tpr_test).max()\n",
    "\n",
    "    #保存ks值\n",
    "    ks_train_lst.append(train_ks)\n",
    "    ks_test_lst.append(test_ks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.5975672844852717,\n 0.5783467616715372,\n 0.5695855263126663,\n 0.5709695906807863,\n 0.5856241417665808]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_train_lst\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.3853595242452872,\n 0.4326870320933379,\n 0.4998707279229346,\n 0.48235428678191344,\n 0.4097696111627038]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_test_lst\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "[0.7537613522256793,\n 0.7771178652722271,\n 0.8143533492088516,\n 0.7938591816154508,\n 0.7697373502589315]"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_list\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "[              importance\n name                    \n td_score              70\n jxl_score             71\n mj_score              71\n rh_score              71\n zzc_score             62\n zcx_score             67\n person_info           33\n finance_info          88\n credit_info           75\n act_info              86,\n               importance\n name                    \n td_score              50\n jxl_score             66\n mj_score              88\n rh_score              62\n zzc_score             62\n zcx_score             66\n person_info           46\n finance_info          86\n credit_info           80\n act_info              88,\n               importance\n name                    \n td_score              68\n jxl_score             64\n mj_score              64\n rh_score              75\n zzc_score             69\n zcx_score             55\n person_info           36\n finance_info          82\n credit_info           91\n act_info              81,\n               importance\n name                    \n td_score              58\n jxl_score             70\n mj_score              79\n rh_score              66\n zzc_score             79\n zcx_score             65\n person_info           54\n finance_info          71\n credit_info           70\n act_info              84,\n               importance\n name                    \n td_score              64\n jxl_score             65\n mj_score              86\n rh_score              78\n zzc_score             77\n zcx_score             73\n person_info           35\n finance_info          70\n credit_info           62\n act_info              85]"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_lst"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "              importance  importance  importance  importance  importance\nname                                                                    \ntd_score              70          50          68          58          64\njxl_score             71          66          64          70          65\nmj_score              71          88          64          79          86\nrh_score              71          62          75          66          78\nzzc_score             62          62          69          79          77\nzcx_score             67          66          55          65          73\nperson_info           33          46          36          54          35\nfinance_info          88          86          82          71          70\ncredit_info           75          80          91          70          62\nact_info              86          88          81          84          85",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>importance</th>\n      <th>importance</th>\n      <th>importance</th>\n      <th>importance</th>\n      <th>importance</th>\n    </tr>\n    <tr>\n      <th>name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>td_score</th>\n      <td>70</td>\n      <td>50</td>\n      <td>68</td>\n      <td>58</td>\n      <td>64</td>\n    </tr>\n    <tr>\n      <th>jxl_score</th>\n      <td>71</td>\n      <td>66</td>\n      <td>64</td>\n      <td>70</td>\n      <td>65</td>\n    </tr>\n    <tr>\n      <th>mj_score</th>\n      <td>71</td>\n      <td>88</td>\n      <td>64</td>\n      <td>79</td>\n      <td>86</td>\n    </tr>\n    <tr>\n      <th>rh_score</th>\n      <td>71</td>\n      <td>62</td>\n      <td>75</td>\n      <td>66</td>\n      <td>78</td>\n    </tr>\n    <tr>\n      <th>zzc_score</th>\n      <td>62</td>\n      <td>62</td>\n      <td>69</td>\n      <td>79</td>\n      <td>77</td>\n    </tr>\n    <tr>\n      <th>zcx_score</th>\n      <td>67</td>\n      <td>66</td>\n      <td>55</td>\n      <td>65</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>person_info</th>\n      <td>33</td>\n      <td>46</td>\n      <td>36</td>\n      <td>54</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>finance_info</th>\n      <td>88</td>\n      <td>86</td>\n      <td>82</td>\n      <td>71</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>credit_info</th>\n      <td>75</td>\n      <td>80</td>\n      <td>91</td>\n      <td>70</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>act_info</th>\n      <td>86</td>\n      <td>88</td>\n      <td>81</td>\n      <td>84</td>\n      <td>85</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(feature_importance_lst,axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "#取特征重要性的平均结果\n",
    "feature_importance = pd.concat(feature_importance_lst,axis=1).mean(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "name\nact_info        84.8\nfinance_info    79.4\nmj_score        77.6\ncredit_info     75.6\nrh_score        70.4\nzzc_score       69.8\njxl_score       67.2\nzcx_score       65.2\ntd_score        62.0\nperson_info     40.8\ndtype: float64"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = feature_importance.sort_values(ascending=False)\n",
    "feature_importance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Number of positive: 1470, number of negative: 78361\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 125\n",
      "[LightGBM] [Info] Number of data points in the train set: 79831, number of used features: 4\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.018414 -> initscore=-3.976064\n",
      "[LightGBM] [Info] Start training from score -3.976064\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "lst = ['person_info','finance_info','credit_info','act_info']\n",
    "#划分训练集和验证集\n",
    "train = data[data['obs_mth'] != '2018-11-30'].reset_index().copy()\n",
    "evl = data[data['obs_mth'] == '2018-11-30'].reset_index().copy()\n",
    "\n",
    "#准备训练集和测试集的x、y\n",
    "x = train[lst]\n",
    "y = train['bad_ind']\n",
    "\n",
    "evl_x = evl[lst]\n",
    "evl_y = evl['bad_ind']\n",
    "\n",
    "#模型训练\n",
    "model ,auc = lgb_test(x,y,evl_x,evl_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7787647675000429"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "train_ks: 0.5112871467802036\n"
     ]
    }
   ],
   "source": [
    "#模型预测，计算ks\n",
    "y_pred = model.predict_proba(x)[:,1]\n",
    "fpr_lgb_train, tpr_lgb_train, _ = roc_curve(y,y_pred)\n",
    "train_ks = abs(fpr_lgb_train - tpr_lgb_train).max()\n",
    "print('train_ks:',train_ks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "evl_ks: 0.4302554685929041\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_proba(evl_x)[:,1]\n",
    "fpr_lgb, tpr_lgb, _ = roc_curve(evl_y,y_pred)\n",
    "evl_ks = abs(fpr_lgb - tpr_lgb).max()\n",
    "print('evl_ks:',evl_ks)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(fpr_lgb_train,tpr_lgb_train,label = 'train LR')\n",
    "plt.plot(fpr_lgb,tpr_lgb,label = 'evl LR')\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 评分映射\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n"
     ]
    },
    {
     "data": {
      "text/plain": "       index     obs_mth  bad_ind                     uid  td_score  \\\n0      79831  2018-11-30      0.0               A10002345  0.123276   \n1      79832  2018-11-30      0.0               A10003755  0.462460   \n2      79833  2018-11-30      0.0                A1000756  0.812642   \n3      79834  2018-11-30      0.0                 A100085  0.007039   \n4      79835  2018-11-30      0.0               A10008856  0.078063   \n...      ...         ...      ...                     ...       ...   \n15970  95801  2018-11-30      0.0  Ab99_96436391998107976  0.890233   \n15971  95802  2018-11-30      0.0  Ab99_96436391998176292  0.161840   \n15972  95803  2018-11-30      0.0  Ab99_96436391998322771  0.746522   \n15973  95804  2018-11-30      0.0  Ab99_96436391998973383  0.176846   \n15974  95805  2018-11-30      0.0  Ab99_96436392001380983  0.417920   \n\n       jxl_score  mj_score  rh_score  zzc_score  zcx_score  person_info  \\\n0       0.872117  0.723560  0.759074   0.184735   0.080376    -0.053718   \n1       0.157643  0.762271  0.481466   0.967006   0.780087     0.013863   \n2       0.400040  0.280942  0.099454   0.942880   0.588936     0.078853   \n3       0.396036  0.857868  0.882255   0.345511   0.419969    -0.053718   \n4       0.291289  0.654864  0.528708   0.754482   0.732534     0.013863   \n...          ...       ...       ...        ...        ...          ...   \n15970   0.442687  0.802687  0.776982   0.638971   0.605522     0.078853   \n15971   0.495766  0.085750  0.536738   0.596144   0.132972     0.078853   \n15972   0.732739  0.025475  0.831805   0.642904   0.029297     0.078853   \n15973   0.749610  0.933879  0.506921   0.867099   0.751643     0.078853   \n15974   0.650343  0.985863  0.374100   0.330634   0.596833     0.078853   \n\n       finance_info  credit_info  act_info     xbeta  \n0          0.047619         1.00  0.230769  0.104260  \n1          0.023810         0.00  0.230769  0.004736  \n2          0.023810         0.02  0.474359  0.016706  \n3          0.047619         0.02  0.666667  0.010245  \n4          0.023810         0.00  0.230769  0.004736  \n...             ...          ...       ...       ...  \n15970      0.142857         0.25  0.076923  0.085003  \n15971      0.023810         0.00  0.076923  0.015709  \n15972      0.023810         0.00  0.076923  0.015709  \n15973      0.023810         0.02  0.076923  0.022099  \n15974      0.071429         0.62  0.076923  0.055635  \n\n[15975 rows x 15 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>obs_mth</th>\n      <th>bad_ind</th>\n      <th>uid</th>\n      <th>td_score</th>\n      <th>jxl_score</th>\n      <th>mj_score</th>\n      <th>rh_score</th>\n      <th>zzc_score</th>\n      <th>zcx_score</th>\n      <th>person_info</th>\n      <th>finance_info</th>\n      <th>credit_info</th>\n      <th>act_info</th>\n      <th>xbeta</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>79831</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A10002345</td>\n      <td>0.123276</td>\n      <td>0.872117</td>\n      <td>0.723560</td>\n      <td>0.759074</td>\n      <td>0.184735</td>\n      <td>0.080376</td>\n      <td>-0.053718</td>\n      <td>0.047619</td>\n      <td>1.00</td>\n      <td>0.230769</td>\n      <td>0.104260</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>79832</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A10003755</td>\n      <td>0.462460</td>\n      <td>0.157643</td>\n      <td>0.762271</td>\n      <td>0.481466</td>\n      <td>0.967006</td>\n      <td>0.780087</td>\n      <td>0.013863</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.230769</td>\n      <td>0.004736</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>79833</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A1000756</td>\n      <td>0.812642</td>\n      <td>0.400040</td>\n      <td>0.280942</td>\n      <td>0.099454</td>\n      <td>0.942880</td>\n      <td>0.588936</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.02</td>\n      <td>0.474359</td>\n      <td>0.016706</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>79834</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A100085</td>\n      <td>0.007039</td>\n      <td>0.396036</td>\n      <td>0.857868</td>\n      <td>0.882255</td>\n      <td>0.345511</td>\n      <td>0.419969</td>\n      <td>-0.053718</td>\n      <td>0.047619</td>\n      <td>0.02</td>\n      <td>0.666667</td>\n      <td>0.010245</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>79835</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A10008856</td>\n      <td>0.078063</td>\n      <td>0.291289</td>\n      <td>0.654864</td>\n      <td>0.528708</td>\n      <td>0.754482</td>\n      <td>0.732534</td>\n      <td>0.013863</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.230769</td>\n      <td>0.004736</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15970</th>\n      <td>95801</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436391998107976</td>\n      <td>0.890233</td>\n      <td>0.442687</td>\n      <td>0.802687</td>\n      <td>0.776982</td>\n      <td>0.638971</td>\n      <td>0.605522</td>\n      <td>0.078853</td>\n      <td>0.142857</td>\n      <td>0.25</td>\n      <td>0.076923</td>\n      <td>0.085003</td>\n    </tr>\n    <tr>\n      <th>15971</th>\n      <td>95802</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436391998176292</td>\n      <td>0.161840</td>\n      <td>0.495766</td>\n      <td>0.085750</td>\n      <td>0.536738</td>\n      <td>0.596144</td>\n      <td>0.132972</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.076923</td>\n      <td>0.015709</td>\n    </tr>\n    <tr>\n      <th>15972</th>\n      <td>95803</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436391998322771</td>\n      <td>0.746522</td>\n      <td>0.732739</td>\n      <td>0.025475</td>\n      <td>0.831805</td>\n      <td>0.642904</td>\n      <td>0.029297</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.076923</td>\n      <td>0.015709</td>\n    </tr>\n    <tr>\n      <th>15973</th>\n      <td>95804</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436391998973383</td>\n      <td>0.176846</td>\n      <td>0.749610</td>\n      <td>0.933879</td>\n      <td>0.506921</td>\n      <td>0.867099</td>\n      <td>0.751643</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.02</td>\n      <td>0.076923</td>\n      <td>0.022099</td>\n    </tr>\n    <tr>\n      <th>15974</th>\n      <td>95805</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436392001380983</td>\n      <td>0.417920</td>\n      <td>0.650343</td>\n      <td>0.985863</td>\n      <td>0.374100</td>\n      <td>0.330634</td>\n      <td>0.596833</td>\n      <td>0.078853</td>\n      <td>0.071429</td>\n      <td>0.62</td>\n      <td>0.076923</td>\n      <td>0.055635</td>\n    </tr>\n  </tbody>\n</table>\n<p>15975 rows × 15 columns</p>\n</div>"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evl['xbeta'] = model.predict_proba(evl_x)[:,1]\n",
    "evl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "       index     obs_mth  bad_ind                     uid  td_score  \\\n0      79831  2018-11-30      0.0               A10002345  0.123276   \n1      79832  2018-11-30      0.0               A10003755  0.462460   \n2      79833  2018-11-30      0.0                A1000756  0.812642   \n3      79834  2018-11-30      0.0                 A100085  0.007039   \n4      79835  2018-11-30      0.0               A10008856  0.078063   \n...      ...         ...      ...                     ...       ...   \n15970  95801  2018-11-30      0.0  Ab99_96436391998107976  0.890233   \n15971  95802  2018-11-30      0.0  Ab99_96436391998176292  0.161840   \n15972  95803  2018-11-30      0.0  Ab99_96436391998322771  0.746522   \n15973  95804  2018-11-30      0.0  Ab99_96436391998973383  0.176846   \n15974  95805  2018-11-30      0.0  Ab99_96436392001380983  0.417920   \n\n       jxl_score  mj_score  rh_score  zzc_score  zcx_score  person_info  \\\n0       0.872117  0.723560  0.759074   0.184735   0.080376    -0.053718   \n1       0.157643  0.762271  0.481466   0.967006   0.780087     0.013863   \n2       0.400040  0.280942  0.099454   0.942880   0.588936     0.078853   \n3       0.396036  0.857868  0.882255   0.345511   0.419969    -0.053718   \n4       0.291289  0.654864  0.528708   0.754482   0.732534     0.013863   \n...          ...       ...       ...        ...        ...          ...   \n15970   0.442687  0.802687  0.776982   0.638971   0.605522     0.078853   \n15971   0.495766  0.085750  0.536738   0.596144   0.132972     0.078853   \n15972   0.732739  0.025475  0.831805   0.642904   0.029297     0.078853   \n15973   0.749610  0.933879  0.506921   0.867099   0.751643     0.078853   \n15974   0.650343  0.985863  0.374100   0.330634   0.596833     0.078853   \n\n       finance_info  credit_info  act_info     xbeta       score  \n0          0.047619         1.00  0.230769  0.104260  755.145015  \n1          0.023810         0.00  0.230769  0.004736  985.762236  \n2          0.023810         0.02  0.474359  0.016706  893.960961  \n3          0.047619         0.02  0.666667  0.010245  929.703744  \n4          0.023810         0.00  0.230769  0.004736  985.762236  \n...             ...          ...       ...       ...         ...  \n15970      0.142857         0.25  0.076923  0.085003  771.409354  \n15971      0.023810         0.00  0.076923  0.015709  898.471700  \n15972      0.023810         0.00  0.076923  0.015709  898.471700  \n15973      0.023810         0.02  0.076923  0.022099  873.382789  \n15974      0.071429         0.62  0.076923  0.055635  804.264302  \n\n[15975 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>obs_mth</th>\n      <th>bad_ind</th>\n      <th>uid</th>\n      <th>td_score</th>\n      <th>jxl_score</th>\n      <th>mj_score</th>\n      <th>rh_score</th>\n      <th>zzc_score</th>\n      <th>zcx_score</th>\n      <th>person_info</th>\n      <th>finance_info</th>\n      <th>credit_info</th>\n      <th>act_info</th>\n      <th>xbeta</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>79831</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A10002345</td>\n      <td>0.123276</td>\n      <td>0.872117</td>\n      <td>0.723560</td>\n      <td>0.759074</td>\n      <td>0.184735</td>\n      <td>0.080376</td>\n      <td>-0.053718</td>\n      <td>0.047619</td>\n      <td>1.00</td>\n      <td>0.230769</td>\n      <td>0.104260</td>\n      <td>755.145015</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>79832</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A10003755</td>\n      <td>0.462460</td>\n      <td>0.157643</td>\n      <td>0.762271</td>\n      <td>0.481466</td>\n      <td>0.967006</td>\n      <td>0.780087</td>\n      <td>0.013863</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.230769</td>\n      <td>0.004736</td>\n      <td>985.762236</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>79833</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A1000756</td>\n      <td>0.812642</td>\n      <td>0.400040</td>\n      <td>0.280942</td>\n      <td>0.099454</td>\n      <td>0.942880</td>\n      <td>0.588936</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.02</td>\n      <td>0.474359</td>\n      <td>0.016706</td>\n      <td>893.960961</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>79834</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A100085</td>\n      <td>0.007039</td>\n      <td>0.396036</td>\n      <td>0.857868</td>\n      <td>0.882255</td>\n      <td>0.345511</td>\n      <td>0.419969</td>\n      <td>-0.053718</td>\n      <td>0.047619</td>\n      <td>0.02</td>\n      <td>0.666667</td>\n      <td>0.010245</td>\n      <td>929.703744</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>79835</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>A10008856</td>\n      <td>0.078063</td>\n      <td>0.291289</td>\n      <td>0.654864</td>\n      <td>0.528708</td>\n      <td>0.754482</td>\n      <td>0.732534</td>\n      <td>0.013863</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.230769</td>\n      <td>0.004736</td>\n      <td>985.762236</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15970</th>\n      <td>95801</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436391998107976</td>\n      <td>0.890233</td>\n      <td>0.442687</td>\n      <td>0.802687</td>\n      <td>0.776982</td>\n      <td>0.638971</td>\n      <td>0.605522</td>\n      <td>0.078853</td>\n      <td>0.142857</td>\n      <td>0.25</td>\n      <td>0.076923</td>\n      <td>0.085003</td>\n      <td>771.409354</td>\n    </tr>\n    <tr>\n      <th>15971</th>\n      <td>95802</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436391998176292</td>\n      <td>0.161840</td>\n      <td>0.495766</td>\n      <td>0.085750</td>\n      <td>0.536738</td>\n      <td>0.596144</td>\n      <td>0.132972</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.076923</td>\n      <td>0.015709</td>\n      <td>898.471700</td>\n    </tr>\n    <tr>\n      <th>15972</th>\n      <td>95803</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436391998322771</td>\n      <td>0.746522</td>\n      <td>0.732739</td>\n      <td>0.025475</td>\n      <td>0.831805</td>\n      <td>0.642904</td>\n      <td>0.029297</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.00</td>\n      <td>0.076923</td>\n      <td>0.015709</td>\n      <td>898.471700</td>\n    </tr>\n    <tr>\n      <th>15973</th>\n      <td>95804</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436391998973383</td>\n      <td>0.176846</td>\n      <td>0.749610</td>\n      <td>0.933879</td>\n      <td>0.506921</td>\n      <td>0.867099</td>\n      <td>0.751643</td>\n      <td>0.078853</td>\n      <td>0.023810</td>\n      <td>0.02</td>\n      <td>0.076923</td>\n      <td>0.022099</td>\n      <td>873.382789</td>\n    </tr>\n    <tr>\n      <th>15974</th>\n      <td>95805</td>\n      <td>2018-11-30</td>\n      <td>0.0</td>\n      <td>Ab99_96436392001380983</td>\n      <td>0.417920</td>\n      <td>0.650343</td>\n      <td>0.985863</td>\n      <td>0.374100</td>\n      <td>0.330634</td>\n      <td>0.596833</td>\n      <td>0.078853</td>\n      <td>0.071429</td>\n      <td>0.62</td>\n      <td>0.076923</td>\n      <td>0.055635</td>\n      <td>804.264302</td>\n    </tr>\n  </tbody>\n</table>\n<p>15975 rows × 16 columns</p>\n</div>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#自定义评分映射函数\n",
    "def score(xbeta):\n",
    "    #score  = 600 + 50log2(P正样本/P负样本)\n",
    "    #正样本：不违约概率，负样本：违约概率\n",
    "    score = 600 + 50 * math.log2((1 - xbeta)/xbeta)\n",
    "    return score\n",
    "evl['score'] = evl.apply(lambda x:score(x['xbeta']),axis=1)\n",
    "evl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "0.4302554685929041"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算验证集上的ks值\n",
    "fpr,tpr,_ = roc_curve(evl_y,evl['score'])\n",
    "val_ks = abs(fpr - tpr).max()\n",
    "val_ks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 模型报告\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n"
     ]
    },
    {
     "data": {
      "text/plain": "     BAD  GOOD  BAD_CNT  GOOD_CNT  BAD_PCTG  BADRATE     KS\nnum                                                        \n0     94   705       94       705     0.287    0.118  0.242\n1     32   767      126      1472     0.384    0.040  0.290\n2     29   770      155      2242     0.473    0.036  0.329\n3     31   767      186      3009     0.567    0.039  0.375\n4     28   771      214      3780     0.652    0.035  0.411\n5     22   777      236      4557     0.720    0.028  0.428\n6      9   789      245      5346     0.747    0.011  0.405\n7     16   783      261      6129     0.796    0.020  0.404\n8      9   790      270      6919     0.823    0.011  0.381\n9     15   784      285      7703     0.869    0.019  0.377\n10     9   789      294      8492     0.896    0.011  0.354\n11     4   795      298      9287     0.909    0.005  0.315\n12     9   790      307     10077     0.936    0.011  0.292\n13     3   795      310     10872     0.945    0.004  0.250\n14     4   795      314     11667     0.957    0.005  0.212\n15     4   795      318     12462     0.970    0.005  0.173\n16     5   793      323     13255     0.985    0.006  0.138\n17     2   797      325     14052     0.991    0.003  0.093\n18     2   797      327     14849     0.997    0.003  0.048\n19     1   798      328     15647     1.000    0.001  0.000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BAD</th>\n      <th>GOOD</th>\n      <th>BAD_CNT</th>\n      <th>GOOD_CNT</th>\n      <th>BAD_PCTG</th>\n      <th>BADRATE</th>\n      <th>KS</th>\n    </tr>\n    <tr>\n      <th>num</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>94</td>\n      <td>705</td>\n      <td>94</td>\n      <td>705</td>\n      <td>0.287</td>\n      <td>0.118</td>\n      <td>0.242</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>32</td>\n      <td>767</td>\n      <td>126</td>\n      <td>1472</td>\n      <td>0.384</td>\n      <td>0.040</td>\n      <td>0.290</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>29</td>\n      <td>770</td>\n      <td>155</td>\n      <td>2242</td>\n      <td>0.473</td>\n      <td>0.036</td>\n      <td>0.329</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>31</td>\n      <td>767</td>\n      <td>186</td>\n      <td>3009</td>\n      <td>0.567</td>\n      <td>0.039</td>\n      <td>0.375</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>28</td>\n      <td>771</td>\n      <td>214</td>\n      <td>3780</td>\n      <td>0.652</td>\n      <td>0.035</td>\n      <td>0.411</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>22</td>\n      <td>777</td>\n      <td>236</td>\n      <td>4557</td>\n      <td>0.720</td>\n      <td>0.028</td>\n      <td>0.428</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>9</td>\n      <td>789</td>\n      <td>245</td>\n      <td>5346</td>\n      <td>0.747</td>\n      <td>0.011</td>\n      <td>0.405</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>16</td>\n      <td>783</td>\n      <td>261</td>\n      <td>6129</td>\n      <td>0.796</td>\n      <td>0.020</td>\n      <td>0.404</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>790</td>\n      <td>270</td>\n      <td>6919</td>\n      <td>0.823</td>\n      <td>0.011</td>\n      <td>0.381</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>784</td>\n      <td>285</td>\n      <td>7703</td>\n      <td>0.869</td>\n      <td>0.019</td>\n      <td>0.377</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>9</td>\n      <td>789</td>\n      <td>294</td>\n      <td>8492</td>\n      <td>0.896</td>\n      <td>0.011</td>\n      <td>0.354</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>4</td>\n      <td>795</td>\n      <td>298</td>\n      <td>9287</td>\n      <td>0.909</td>\n      <td>0.005</td>\n      <td>0.315</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>9</td>\n      <td>790</td>\n      <td>307</td>\n      <td>10077</td>\n      <td>0.936</td>\n      <td>0.011</td>\n      <td>0.292</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>3</td>\n      <td>795</td>\n      <td>310</td>\n      <td>10872</td>\n      <td>0.945</td>\n      <td>0.004</td>\n      <td>0.250</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>4</td>\n      <td>795</td>\n      <td>314</td>\n      <td>11667</td>\n      <td>0.957</td>\n      <td>0.005</td>\n      <td>0.212</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>4</td>\n      <td>795</td>\n      <td>318</td>\n      <td>12462</td>\n      <td>0.970</td>\n      <td>0.005</td>\n      <td>0.173</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>5</td>\n      <td>793</td>\n      <td>323</td>\n      <td>13255</td>\n      <td>0.985</td>\n      <td>0.006</td>\n      <td>0.138</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2</td>\n      <td>797</td>\n      <td>325</td>\n      <td>14052</td>\n      <td>0.991</td>\n      <td>0.003</td>\n      <td>0.093</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2</td>\n      <td>797</td>\n      <td>327</td>\n      <td>14849</td>\n      <td>0.997</td>\n      <td>0.003</td>\n      <td>0.048</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1</td>\n      <td>798</td>\n      <td>328</td>\n      <td>15647</td>\n      <td>1.000</td>\n      <td>0.001</td>\n      <td>0.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把验证集的数据代入到训练好的模型, 输出违约率, 把所有的用户的违约率按从大到小排序, 然后均匀划分成20箱\n",
    "# 计算每一箱的跟违约相关的指标, 违约率和 KS\n",
    "bins = 20\n",
    "temp_df = pd.DataFrame() # 准备空白的df\n",
    "# 用训练好的模型, 输出测试集的违约率\n",
    "temp_df['bad_rate_predict'] = model.predict_proba(evl_x)[:,1] # 模型预测的违约率\n",
    "temp_df['real_bad']= evl_y.values # 真实的标签\n",
    "\n",
    "temp_df = temp_df.sort_values('bad_rate_predict',ascending=False)\n",
    "\n",
    "temp_df['num'] = [i for i in range(temp_df.shape[0])]\n",
    "temp_df['num'] = pd.cut(temp_df['num'],bins = bins,labels=[i for i in range(bins)])\n",
    "\n",
    "\n",
    "\n",
    "# 创建报告\n",
    "report = pd.DataFrame()\n",
    "# 每一组有多少1 bad标签 , 每一组有多少0 good标签\n",
    "report['BAD'] =temp_df.groupby('num')['real_bad'].sum().astype(int)\n",
    "report['GOOD'] =temp_df.groupby('num')['real_bad'].count().astype(int)-report['BAD']\n",
    "# 累计求和 累计到这一组, 有多少1, 多少0\n",
    "report['BAD_CNT'] = report['BAD'].cumsum()\n",
    "report['GOOD_CNT'] = report['GOOD'].cumsum()\n",
    "# 计算累计到当前组, 出现的1标签的比例\n",
    "good_total = report['GOOD_CNT'].max()\n",
    "bad_total = report['BAD_CNT'].max()\n",
    "report['BAD_PCTG'] = round(report['BAD_CNT']/bad_total,3)\n",
    "# 当前组 1标签比例\n",
    "report['BADRATE'] = report.apply(lambda x:round(x['BAD']/(x['BAD']+x['GOOD']),3),axis = 1)\n",
    "# 当前组ks\n",
    "def cal_ks(x):\n",
    "\t# tpr = tp/tp+fn(所有的1标签)  fpr = fp/fp+tn (所有的0) GOOD_CNT\n",
    "\tks = (x['BAD_CNT']/bad_total)-(x['GOOD_CNT']/good_total)\n",
    "\treturn round(abs(ks),3)\n",
    "report['KS'] = report.apply(cal_ks,axis = 1)\n",
    "report\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "'D:\\\\code\\\\workspace2\\\\financial20\\\\day06\\\\render.html'"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyecharts.charts import *\n",
    "from pyecharts import options as opts\n",
    "from pylab import *\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.unicode.ambiguous_as_wide', True)\n",
    "pd.set_option('display.unicode.east_asian_width', True)\n",
    "line = (\n",
    "\n",
    "    Line()\n",
    "    .add_xaxis(list(report.index))\n",
    "    .add_yaxis(\n",
    "        \"分组坏人占比\",\n",
    "        list(report.BADRATE),\n",
    "        yaxis_index=0,\n",
    "        color=\"red\",\n",
    "    )\n",
    "    .set_global_opts(\n",
    "        title_opts=opts.TitleOpts(title=\"评分卡模型表现\"),\n",
    "    )\n",
    "    .extend_axis(\n",
    "        yaxis=opts.AxisOpts(\n",
    "            name=\"累计坏人占比\",\n",
    "            type_=\"value\",\n",
    "            min_=0,\n",
    "            max_=0.5,\n",
    "            position=\"right\",\n",
    "            axisline_opts=opts.AxisLineOpts(\n",
    "                linestyle_opts=opts.LineStyleOpts(color=\"red\")\n",
    "            ),\n",
    "            axislabel_opts=opts.LabelOpts(formatter=\"{value}\"),\n",
    "        )\n",
    "\n",
    "    )\n",
    "    .add_yaxis(\n",
    "        \"KS\",\n",
    "        list(report['KS']),\n",
    "        yaxis_index=1,\n",
    "        color=\"blue\",\n",
    "        label_opts=opts.LabelOpts(is_show=False),\n",
    "    )\n",
    ")\n",
    "line.render()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}