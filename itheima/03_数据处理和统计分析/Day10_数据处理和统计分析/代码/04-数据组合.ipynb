{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 导包"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ad9ca44e2a29a0f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c89a70ef9e2a904a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 数据组合-concat()函数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "123377b5d9e6aeeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. 加载数据源, 获取df对象.\n",
    "df1 = pd.read_csv('data/concat_1.csv')\n",
    "df2 = pd.read_csv('data/concat_2.csv')\n",
    "df3 = pd.read_csv('data/concat_3.csv')\n",
    "df1\n",
    "df2\n",
    "df3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21c3a01c725fb96f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. concat()函数, 把df对象连接起来, 默认是: 按行拼接.\n",
    "# 格式: pd.concat([df1, df2, df3...])\n",
    "# 细节: concat()函数, 按行拼接是, 参考: 列名,  按列拼接是, 参考: 行索引\n",
    "pd.concat([df1, df2, df3])  # 按行拼接\n",
    "pd.concat([df1, df2, df3], axis='rows')  # 按行拼接, 效果同上\n",
    "pd.concat([df1, df2, df3], axis=0)  # 按行拼接, 效果同上\n",
    "\n",
    "pd.concat([df1, df2, df3], axis='columns')  # 按列拼接\n",
    "pd.concat([df1, df2, df3], axis=1)  # 按列拼接, 效果同上"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e3715ef9650008c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. 把DataFrame 和 Series对象拼接到一起.\n",
    "# 细节: 由于Series是列数据, concat()方法默认是添加行, 但是Series中没有行索引, 所以添加了新的列, 缺失值用NaN条虫. \n",
    "s1 = pd.Series(['n1', 'n2', 'n3'])\n",
    "pd.concat([df1, s1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f4786534488ff0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. 如果想将 ['n1', 'n2', 'n3', 'n4'] 作为 行链接到 df1后, 如何实现.\n",
    "df5 = pd.DataFrame([['n1', 'n2', 'n3', 'n4']], columns=['A', 'B', 'C', 'D'])  # 按行拼接, 参考: 列名,  按列拼接, 参考: 行索引\n",
    "df5\n",
    "\n",
    "pd.concat([df1, df5], ignore_index=True)  # 忽略行索引, 即: 会自动重置. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd45e07f28643b70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. 演示: append()函数, 注意: 该函数已过时, 新版本中已移除它. \n",
    "df1.append(df2, ignore_index=True)\n",
    "\n",
    "# 使用python字段, 添加数据行\n",
    "dict1 = {'A': 'n1', 'B': 'n2', 'C': 'n3', 'D': 'n4'}\n",
    "dict1\n",
    "\n",
    "df1.append(dict1, ignore_index=True)  # 把字典元素 拼接到 df1对象中. \n",
    "# df1.append(dict1)                     # 报错, 和字典拼接的时候, 必须写 ignore_index=True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d018679b0d84e1f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 6. 添加列. \n",
    "pd.concat([df1, df2], axis='columns')\n",
    "pd.concat([df1, df2], axis=1)           # 效果同上.\n",
    "# pd.concat([df1, df2], axis=1, ignore_index=True)           # 效果同上.\n",
    "\n",
    "# df[列名] = 列值  这个方式也可以给df新增1列\n",
    "df1['new_col'] = 'ai20'\n",
    "df1\n",
    "\n",
    "df1['new_col2'] = ['张三', '李四', '王五', '赵六']\n",
    "df1\n",
    "\n",
    "# 还可以吧要添加的列值, 封装成: Series对象, 因为: Series对象就是一维数组, 代表着: 某行, 或者某列数据\n",
    "df1['new_col3'] = pd.Series(['刘亦菲', '胡歌', '高圆圆', '光良'])\n",
    "df1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31adc766753d9e52"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 数据组合-merge()函数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb16e08f1955bbdc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 一对一合并"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c166c4460522ac31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. 从sqlite中, 读取数据源. \n",
    "conn = sqlite3.connect('data/chinook.db')\n",
    "tracks = pd.read_sql_query('select * from tracks;', conn)    # 歌曲表\n",
    "tracks.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "386d961f6307f722"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "genres = pd.read_sql_query('select * from genres;', conn)   # 歌曲分类表\n",
    "genres.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3c3dd0e2c1328a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. 为了更好的演示 连接查询, 防止不相关的列来干扰我们, 我们从 tracks(歌曲表中)抽取一些数据.\n",
    "# 从track表(歌曲表)提取部分数据, 使其不含重复的'GenreID'值\n",
    "tracks_subset = tracks.loc[[0, 62, 76, 98, 110, 193, 204, 281, 322, 359], ]\n",
    "tracks_subset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "664977ade0381119"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. 通过 merge()函数, 实现 tracks_subset(歌曲表子集) 和 genres(歌曲分类表) 连接操作.\n",
    "# 格式: df.merge(df, on='关联字段', how='连接方式')\n",
    "# 细节1: 如果两个df对象关联字段一样, 用on直接连接.   如果不一样, 用 left_on='左df字段名', right_on='右df字段名'\n",
    "genres.merge(tracks_subset[['TrackId', 'GenreId', 'Milliseconds']], on='GenreId', how='left')       # left = 左外连接, 左表全集 + 交集.  \n",
    "genres.merge(tracks_subset[['TrackId', 'GenreId', 'Milliseconds']], on='GenreId', how='right')      # right = 右外连接, 右表全集 + 交集.  \n",
    "genres.merge(tracks_subset[['TrackId', 'GenreId', 'Milliseconds']], on='GenreId', how='inner')      # inner = 内连接, 交集.  \n",
    "genres.merge(tracks_subset[['TrackId', 'GenreId', 'Milliseconds']], on='GenreId', how='outer')      # outer = 满外连接(左外连接 + 右外连接), 交集."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e7647a2f2e2f52f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 细节2: 如果两个df的字段重名了, 则 suffixes=('_x', '_y')会分别给 左df 和 右df加后缀, 以示区分.\n",
    "# genres.merge(tracks_subset[['TrackId', 'Name', 'GenreId', 'Milliseconds']], on='GenreId', how='outer', suffixes=('_左表', '_右表')) \n",
    "genres.merge(tracks_subset[['TrackId', 'Name', 'GenreId', 'Milliseconds']], on='GenreId', how='outer') "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46853ad0423169b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 多对一合并"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79a35b35cf948865"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "GenreId  Name              \n5        Rock And Roll        0 days 00:02:14\n25       Opera                0 days 00:02:54\n17       Hip Hop/Rap          0 days 00:02:58\n12       Easy Listening       0 days 00:03:09\n11       Bossa Nova           0 days 00:03:39\n14       R&B/Soul             0 days 00:03:40\n16       World                0 days 00:03:44\n9        Pop                  0 days 00:03:49\n7        Latin                0 days 00:03:52\n4        Alternative & Punk   0 days 00:03:54\n10       Soundtrack           0 days 00:04:04\n8        Reggae               0 days 00:04:07\n23       Alternative          0 days 00:04:24\n6        Blues                0 days 00:04:30\n1        Rock                 0 days 00:04:43\n2        Jazz                 0 days 00:04:51\n24       Classical            0 days 00:04:53\n13       Heavy Metal          0 days 00:04:57\n15       Electronica/Dance    0 days 00:05:02\n3        Metal                0 days 00:05:09\n22       Comedy               0 days 00:26:25\n19       TV Shows             0 days 00:35:45\n21       Drama                0 days 00:42:55\n18       Science Fiction      0 days 00:43:45\n20       Sci Fi & Fantasy     0 days 00:48:31\nName: Milliseconds, dtype: timedelta64[ns]"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 需求: 计算 每个类别 歌曲的平均时长.\n",
    "# 1. 把 歌曲表 tracks 和 歌曲类别表 genres关联到一起.\n",
    "# genres.merge(tracks_subset)         # 一对一, 因为 tracks_subset(歌曲表子集)中只有10条数据, 且 歌曲分类id都是不同的. \n",
    "# genres.merge(tracks)                # 一对多, 因为 tracks(歌曲表)中 多首歌曲  有可能属于 同一个类别.  \n",
    "genre_track = genres.merge(tracks[['TrackId', 'GenreId', 'Milliseconds']], on='GenreId', how='left')\n",
    "\n",
    "# 2. 基于上述的数据, 按照 歌曲类别分组, 计算 平均时长.\n",
    "tmp_series = genre_track.groupby(['GenreId', 'Name'])['Milliseconds'].mean()\n",
    "tmp_series\n",
    "\n",
    "# 3. 基于上述的数据, 转成 日期格式.\n",
    "# pd.to_timedelta(对象, 单位)       把指定的内容(对象) 转成 timedelta 日期类型\n",
    "# dt.floor()                       这里是做截断的意思, s表示秒, 即: 截断到秒级.\n",
    "pd.to_timedelta(tmp_series, unit='ms').dt.floor('s').sort_values()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T08:29:45.786710300Z",
     "start_time": "2024-07-22T08:29:45.738098400Z"
    }
   },
   "id": "a3c768f2458bf6b0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. 数据组合-join函数 (了解)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46ce21dc2faf3e84"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "  Symbol  Shares  Low  High\n0   AAPL      80   95   110\n1   TSLA      50   80   130\n2    WMT      40   55    70",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Symbol</th>\n      <th>Shares</th>\n      <th>Low</th>\n      <th>High</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AAPL</td>\n      <td>80</td>\n      <td>95</td>\n      <td>110</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TSLA</td>\n      <td>50</td>\n      <td>80</td>\n      <td>130</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>WMT</td>\n      <td>40</td>\n      <td>55</td>\n      <td>70</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 读取数据源, 获取df对象.\n",
    "stocks_2016 = pd.read_csv('data/stocks_2016.csv')\n",
    "stocks_2016"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T08:51:37.252752400Z",
     "start_time": "2024-07-22T08:51:37.210096500Z"
    }
   },
   "id": "707307fde027a47c"
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "  Symbol  Shares  Low  High\n0   AAPL      50  120   140\n1     GE     100   30    40\n2    IBM      87   75    95\n3    SLB      20   55    85\n4    TXN     500   15    23\n5   TSLA     100  100   300",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Symbol</th>\n      <th>Shares</th>\n      <th>Low</th>\n      <th>High</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AAPL</td>\n      <td>50</td>\n      <td>120</td>\n      <td>140</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GE</td>\n      <td>100</td>\n      <td>30</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>IBM</td>\n      <td>87</td>\n      <td>75</td>\n      <td>95</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SLB</td>\n      <td>20</td>\n      <td>55</td>\n      <td>85</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TXN</td>\n      <td>500</td>\n      <td>15</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>TSLA</td>\n      <td>100</td>\n      <td>100</td>\n      <td>300</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_2017 = pd.read_csv('data/stocks_2017.csv')\n",
    "stocks_2017"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T08:51:54.917614100Z",
     "start_time": "2024-07-22T08:51:54.857101600Z"
    }
   },
   "id": "d2539d081b3117fa"
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "  Symbol  Shares  Low  High\n0   AAPL      40  135   170\n1   AMZN       8  900  1125\n2   TSLA      50  220   400",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Symbol</th>\n      <th>Shares</th>\n      <th>Low</th>\n      <th>High</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AAPL</td>\n      <td>40</td>\n      <td>135</td>\n      <td>170</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AMZN</td>\n      <td>8</td>\n      <td>900</td>\n      <td>1125</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TSLA</td>\n      <td>50</td>\n      <td>220</td>\n      <td>400</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_2018 = pd.read_csv('data/stocks_2018.csv')\n",
    "stocks_2018"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T08:52:03.268233400Z",
     "start_time": "2024-07-22T08:52:03.216650500Z"
    }
   },
   "id": "f8bfc51f86f28b0e"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "  Symbol  Shares_x  Low_x  High_x  Shares_y  Low_y  High_y\n0   AAPL        80     95     110        40    135     170\n1   TSLA        50     80     130        50    220     400",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Symbol</th>\n      <th>Shares_x</th>\n      <th>Low_x</th>\n      <th>High_x</th>\n      <th>Shares_y</th>\n      <th>Low_y</th>\n      <th>High_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AAPL</td>\n      <td>80</td>\n      <td>95</td>\n      <td>110</td>\n      <td>40</td>\n      <td>135</td>\n      <td>170</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TSLA</td>\n      <td>50</td>\n      <td>80</td>\n      <td>130</td>\n      <td>50</td>\n      <td>220</td>\n      <td>400</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 通过join, 来合并 上述的 df对象.      join: 默认是根据行索引来匹配的, 可以通过 on 设置关联字段. \n",
    "# 场景1: 依据两个df的行索引来合并.\n",
    "stocks_2016.join(stocks_2017, lsuffix='_2016', rsuffix='_2017')                 # 默认是: 左外连接\n",
    "stocks_2016.join(stocks_2017, lsuffix='_2016', rsuffix='_2017', how='outer')    # 指定: 满外连接\n",
    "\n",
    "# 场景2: 两个df的symbol设置为行索引, 然后关联. 无需手动写on字段, 默认就是: 按照行索引关联.\n",
    "stocks_2016.set_index('Symbol').join(stocks_2017.set_index('Symbol'), lsuffix='_2016', rsuffix='_2017')\n",
    "\n",
    "# 场景3: 1个df的Symbol设置为行索引, 另1个df不设置.\n",
    "stocks_2016.join(stocks_2018.set_index('Symbol'), lsuffix='_2016', rsuffix='_2017', on='Symbol')\n",
    "\n",
    "# 上边的事儿, 用: concat(), merge()都能实现.\n",
    "pd.concat([stocks_2016, stocks_2018], axis='columns')\n",
    "pd.concat([stocks_2016, stocks_2018], axis=1)           # 行拼接: 参考列名.  列拼接: 参考行索引, 默认为: outer(满外连接)\n",
    "\n",
    "stocks_2016.merge(stocks_2018, on='Symbol')             # 只能列拼接, 默认为: inner(内连接)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-22T09:03:22.623221100Z",
     "start_time": "2024-07-22T09:03:22.578581300Z"
    }
   },
   "id": "4ac3579f2e151c12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "587fd917afbc98b2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
